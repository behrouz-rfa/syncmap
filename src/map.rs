use std::collections::HashMap;
use std::fmt;
use std::fmt::{Debug, Formatter};
use std::hash::{BuildHasher, Hash};
use std::sync::atomic::{AtomicIsize, AtomicUsize};
use crate::entry::Entry;
use crate::reclaim::Atomic;



pub struct Map<K, V, S = crate::DefaultHashBuilder> {
    read: Atomic<ReadOnly<K, V>>,
    dirty: Atomic<HashMap<K, *mut Entry<V>>>,
    misses: AtomicUsize,
    flag_ctl: AtomicIsize,
    build_hasher: S,
    collector: Collector,

}

impl<K, V, S> fmt::Debug for Map<K, V, S>
    where
        K: Debug,
        V: Debug,
{
    fn fmt(&self, f: &mut Formatter<'_>) -> fmt::Result {
        let guard = self.collector.enter();
        f.debug_map().finish()
    }
}

impl<K, V, S> Clone for Map<K, V, S>
    where
        K: Sync + Send + Clone + Hash + Ord,
        V: Sync + Send + Clone,
        S: BuildHasher + Clone,
{
    fn clone(&self) -> Map<K, V, S> {
        let mut cloned_map = Map::with_hasher(self.build_hasher.clone());

        {
            let guard = self.collector.enter();
            let cloned_guard = cloned_map.collector.enter();
            let dirty = self.dirty.load(Ordering::SeqCst, &guard);
            if !dirty.is_null() {
                for (key, value) in unsafe { dirty.deref() }.deref() {
                    let value = unsafe { (value.as_ref().unwrap()).p.load(Ordering::SeqCst, &guard).deref().deref() };
                    cloned_map.insert(key.clone(), value.clone(), &cloned_guard)
                }
            }
        }
        cloned_map
    }
}

impl<K, V> Map<K, V, crate::DefaultHashBuilder> {
    /// Creates an empty `HashMap`.
    ///
    /// The hash map is initially created with a capacity of 0, so it will not allocate until it
    /// is first inserted into.
    ///
    /// # Examples
    ///
    /// ```
    ///
    /// use syncmap::map::Map;
    /// let map: Map<&str, i32> = Map::new();
    /// ```
    pub fn new() -> Self {
        Self::default()
    }
}

impl<K, V, S> Default for Map<K, V, S>
    where
        S: Default,
{
    fn default() -> Self {
        Self::with_hasher(S::default())
    }
}

impl<K, V, S> Map<K, V, S> {
    /// Creates an empty map which will use `hash_builder` to hash keys.
    ///
    /// The created map has the default initial capacity.
    ///
    /// Warning: `hash_builder` is normally randomly generated, and is designed to
    /// allow the map to be resistant to attacks that cause many collisions and
    /// very poor performance. Setting it manually using this
    /// function can expose a DoS attack vector.
    ///
    /// # Examples
    ///
    /// ```
    ///
    /// use syncmap::map::Map;
    /// let map = Map::with_hasher(DefaultHashBuilder::default());
    /// map.pin().insert(1, 2);
    /// ```
    pub fn with_hasher(hash_builder: S) -> Self {
        Self {
            read: Atomic::null(),
            dirty: Atomic::null(),
            misses: AtomicUsize::new(0),
            flag_ctl: AtomicIsize::new(0),
            build_hasher: hash_builder,
            collector: Collector::new(),
        }
    }

    /// Pin a `Guard` for use with this map.
    ///
    /// Keep in mind that for as long as you hold onto this `Guard`, you are preventing the
    /// collection of garbage generated by the map.
    pub fn guard(&self) -> Guard<'_> {
        self.collector.enter()
    }

    #[inline]
    fn check_guard(&self, guard: &Guard<'_>) {
        // guard.collector() may be `None` if it is unprotected
        if let Some(c) = guard.collector() {
            assert!(Collector::ptr_eq(c, &self.collector));
        }
    }

    fn init_table<'g>(&'g self, guard: &'g Guard<'_>) -> Shared<'g, ReadOnly<K, V>> {
        loop {
            let table = self.read.load(Ordering::SeqCst, guard);
            // safety: we loaded the ReadOnly while the thread was marked as active.
            // ReadOnly won't be deallocated until the guard is dropped at the earliest.
            if !table.is_null() {
                break table;
            }
            //try allocate ReadOnly
            let mut flag = self.flag_ctl.load(Ordering::SeqCst);
            if flag < 0 {
                //lost tje init race; just spin
                std::thread::yield_now();
                continue;
            }

            if self.flag_ctl
                .compare_exchange(flag, -1, Ordering::SeqCst, Ordering::Relaxed).is_ok() {
                let mut table = self.read.load(Ordering::SeqCst, guard);
                if table.is_null() {
                    let n = if flag > 0 {
                        flag as usize
                    } else {
                        1
                    };
                    table = Shared::boxed(ReadOnly::new(), &self.collector);
                    self.read.store(table, Ordering::SeqCst);
                    let m = Shared::boxed(HashMap::new(), &self.collector);
                    self.dirty.store(m, Ordering::SeqCst);
                    flag = load_factor!(n as isize)
                }
                self.flag_ctl.store(flag, Ordering::SeqCst);
                break table;
            }
        }
    }
}